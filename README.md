# Evaluating DeepSeek-R1 Distilled Models on GPQA Using Ollama and OpenAI's simple-evals

## Context
- The recent launch of the DeepSeek-R1 model sent ripples across the global AI community, delivering breakthroughs on par with those from Meta and OpenAI,  achieved in a fraction of the time and at a much lower cost.
- Beyond the headlines and online buzz, how can we assess the model's reasoning abilities using recognized benchmarks?Â 
- DeepSeek's user interface provides an easy way to explore its capabilities, but using it programmatically offers deeper insights and more seamless integration into real-world applications. 
- Understanding how to run such models locally also provides enhanced control and offline access.
- In this article, we will walk through how to use Ollama and OpenAI's simple-evals to evaluate the reasoning capabilities of DeepSeek-R1's distilled models based on the renowned GPQA-Diamond benchmark.

**More details coming soon!**